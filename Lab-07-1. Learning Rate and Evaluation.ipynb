{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab-07-1. Learning Rate and Evaluation.ipynb","provenance":[],"authorship_tag":"ABX9TyO1HtFHkVzBKnuDRG7ZlaPo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_zOVBiTHW2nE","colab_type":"text"},"source":["# Lab-07-1. Learning Rate and Evaluation\n","* Learning Rate와 Evaluation에 대한 설명\n","\n","### 기본 Library 선언 및 TensorFlow 버전 확인"]},{"cell_type":"code","metadata":{"id":"DrFvnGfvWwtZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import tensorflow as tf\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","tf.random.set_seed(777)  # for reproducibility\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-yXai89aW6my","colab_type":"text"},"source":["### 강의에 설명할 Data입니다\n","* x_data가 3차원 배열이기에 3차원 공간에 표현하여 x1과 x2, x3를 기준으로 y_data 3개 클래스를 구분하는 예제입니다\n","* 해당 Data를 3개의 색상으로 구분해서 출력해 보겠습니다.(파랑-2, 초록-1, 빨강-0)"]},{"cell_type":"code","metadata":{"id":"9gieuauEW8HI","colab_type":"code","colab":{}},"source":["x_train = [[1, 2, 1],\n","          [1, 3, 2],\n","          [1, 3, 4],\n","          [1, 5, 5],\n","          [1, 7, 5],\n","          [1, 2, 5],\n","          [1, 6, 6],\n","          [1, 7, 7]]\n","\n","y_train = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [1, 0, 0],\n","          [1, 0, 0]]\n","\n","# Evaluation our model using this test dataset\n","x_test = [[2, 1, 1],\n","          [3, 1, 2],\n","          [3, 3, 4]]\n","y_test = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1]]\n","\n","\n","x1 = [x[0] for x in x_train]\n","x2 = [x[1] for x in x_train]\n","x3 = [x[2] for x in x_train]\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","ax.scatter(x1, x2, x3, c=y_train, marker='^')\n","\n","ax.scatter(x_test[0][0], x_test[0][1], x_test[0][2], c=\"black\", marker='^')\n","ax.scatter(x_test[1][0], x_test[1][1], x_test[1][2], c=\"black\", marker='^')\n","ax.scatter(x_test[2][0], x_test[2][1], x_test[2][2], c=\"black\", marker='^')\n","\n","\n","ax.set_xlabel('X Label')\n","ax.set_ylabel('Y Label')\n","ax.set_zlabel('Z Label')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2x9SD7mXJZD","colab_type":"text"},"source":["##  Tensorflow 2\n","### 위 Data를 기준으로 Learning Rate 값과 평가 모델을 만들도록 하겠습니다\n","* Tensorflow data API를 통해 학습시킬 값들을 담는다 (Batch Size는 한번에 학습시킬 Size로 정한다)\n","* features,labels는 실재 학습에 쓰일 Data (연산을 위해 Type를 맞춰준다)"]},{"cell_type":"code","metadata":{"id":"MbXXsLW8W-5X","colab_type":"code","colab":{}},"source":["dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1kEs8dTXRI8","colab_type":"text"},"source":["### 위 Data를 기준으로 가설의 검증을 통해 Softmax Classification 모델을 만들도록 하겠습니다\n","* W와 b은 학습을 통해 생성되는 모델에 쓰이는 Wegith와 Bias (초기값을 variable : 0이나 Random값으로 가능 tf.random_normal([3, 3],tf.zeros([3,3]) )"]},{"cell_type":"code","metadata":{"id":"b3E8bsBsXS5k","colab_type":"code","colab":{}},"source":["W = tf.Variable(tf.random.normal((3, 3)))\n","b = tf.Variable(tf.random.normal((3,)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YZRQ0R0vXUsD","colab_type":"text"},"source":["### Softmax 함수를 가설로 선언합니다\n","* Softmax를 통해 가장 높은 값을 구한다 (0~1사이의 값 합계는 1)"]},{"cell_type":"code","metadata":{"id":"wamqn-76XWAB","colab_type":"code","colab":{}},"source":["def softmax_fn(features):\n","    hypothesis = tf.nn.softmax(tf.matmul(features, W) + b)\n","    return hypothesis"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"asANNbWJXYPJ","colab_type":"text"},"source":["### 가설을 검증할 Cost 함수를 정의합니다\n","* Cross Entropy loss를 사용"]},{"cell_type":"code","metadata":{"id":"TnQtrWDlXX2i","colab_type":"code","colab":{}},"source":["def loss_fn(hypothesis, features, labels):\n","    cost = tf.reduce_mean(-tf.reduce_sum(labels * tf.math.log(hypothesis), axis=1))\n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtsVGONQXch3","colab_type":"text"},"source":["### Learning Rate 값을 조정하기 위한 Learning Decay 설정 \n","#### 5개 파라미터 설정\n","* starter_learning_rate : 최초 학습시 사용될 learning rate (0.1로 설정하여 0.96씩 감소하는지 확인)\n","* global_step : 현재 학습 횟수\n","* 1000 : 곱할 횟수 정의 (1000번에 마다 적용)\n","* 0.96 : 기존 learning에 곱할 값\n","* 적용유무 decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)"]},{"cell_type":"code","metadata":{"id":"DEgbPWffXgtc","colab_type":"code","colab":{}},"source":["is_decay = True\n","starter_learning_rate = 0.1\n","    \n","if(is_decay):    \n","    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=starter_learning_rate,\n","                                                                 decay_steps=1000,\n","                                                                 decay_rate=0.96,\n","                                                                 staircase=True)\n","    optimizer = tf.keras.optimizers.SGD(learning_rate)\n","else:\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=starter_learning_rate)\n","\n","def grad(hypothesis, features, labels):\n","    with tf.GradientTape() as tape:\n","        loss_value = loss_fn(softmax_fn(features),features,labels)\n","    return tape.gradient(loss_value, [W,b])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wrm82tTKXlBE","colab_type":"text"},"source":["### 가설을 통해 실재 값과 비교한 정확도를 측정합니다"]},{"cell_type":"code","metadata":{"id":"o2bOjdTWXm2S","colab_type":"code","colab":{}},"source":["def accuracy_fn(hypothesis, labels):\n","    prediction = tf.argmax(hypothesis, 1)\n","    is_correct = tf.equal(prediction, tf.argmax(labels, 1))\n","    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XhW9yDAYXobY","colab_type":"text"},"source":["### TensorFlow를 통해 학습을 진행합니다.\n","* 위의 Data를 Cost함수를 통해 학습시킨 후 모델을 생성합니다.\n","* Test Data값 검증 수행 [2, 1, 1], [3, 1, 2], [3, 3, 4]"]},{"cell_type":"code","metadata":{"id":"1iTE3CbGXqeU","colab_type":"code","colab":{}},"source":["EPOCHS = 1001\n","\n","for step in range(EPOCHS):\n","    for features, labels  in iter(dataset):\n","        features = tf.cast(features, tf.float32)\n","        labels = tf.cast(labels, tf.float32)\n","        grads = grad(softmax_fn(features), features, labels)\n","        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n","        if step % 100 == 0:\n","            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(softmax_fn(features),features,labels)))\n","x_test = tf.cast(x_test, tf.float32)\n","y_test = tf.cast(y_test, tf.float32)\n","test_acc = accuracy_fn(softmax_fn(x_test),y_test)\n","print(\"Testset Accuracy: {:.4f}\".format(test_acc))"],"execution_count":null,"outputs":[]}]}